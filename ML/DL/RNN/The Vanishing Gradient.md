- ![[Pasted image 20251114194519.png]]
- # ðŸ§  Vanishing Gradient Problem

## ðŸ” **Definition**

The **vanishing gradient problem** occurs when gradients become extremely small as they propagate backward through a deep neural network.  
As a result, **early layers learn very slowly or not at all**, causing training failure.

---

## ðŸ§ª **Why It Happens**

Most activation functions (especially **sigmoid** and **tanh**) compress inputs into a small range.

- Sigmoid derivative â‰¤ 0.25
    
- Repeated multiplication during backprop:
    

`small_derivative^n â†’ almost 0`

In deep networks, this makes gradients disappear before they reach earlier layers.

---

## âš ï¸ **Consequences**

- Early layers fail to update
    
- Slow or stalled training
    
- Poor model performance
    
- Especially severe in RNNs
    

---

## ðŸ“‰ **Example**

If each layer contributes a derivative â‰ˆ 0.2, then:

`(0.2)^20 â‰ˆ 1.04eâˆ’14 â†’ basically zero`

---

## ðŸ› ï¸ **Solutions**

### âœ”ï¸ ReLU and Variants

- ReLU, LeakyReLU, GELU â†’ avoid tiny derivatives
    

### âœ”ï¸ Better Weight Initialization

- **Xavier (Glorot)** for tanh
    
- **He initialization** for ReLU
    

### âœ”ï¸ Batch Normalization

- Normalizes activations, stabilizes gradients
    

### âœ”ï¸ Residual Connections (ResNet)

- Skip connections allow gradients to flow directly backwards
    
- Solved deep network training up to 1000+ layers
    

### âœ”ï¸ LSTM/GRU (for RNNs)

- Designed specifically to avoid vanishing gradients using gating mechanisms
    

---

## ðŸ§© **TL;DR**

> **Vanishing gradients = training signal dies before reaching the early layers, causing them to stop learning.**
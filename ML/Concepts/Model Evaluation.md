#ML #ModelEvaluation #accuracy #precision #recall #F1score
# üìä Model Evaluation Metrics

Use these metrics to evaluate how well a classification model is performing.

---

## üìå Confusion Matrix

|              | Predicted Positive | Predicted Negative |
|--------------|--------------------|--------------------|
| Actual Positive | ‚úÖ True Positive (TP)  | ‚ùå False Negative (FN) |
| Actual Negative | ‚ùå False Positive (FP) | ‚úÖ True Negative (TN)  |

---

## ‚úÖ Accuracy

- **Formula**:  
  $$
  \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
  $$

- **Meaning**:  
  The proportion of total correct predictions.

- ‚úÖ Good when: Classes are **balanced**.

- ‚ùå Bad when: Classes are **imbalanced** (e.g., 95% Negative, 5% Positive).

---

## üéØ Precision

- **Formula**:  
  $$
  \text{Precision} = \frac{TP}{TP + FP}
  $$

- **Meaning**:  
  Out of all predicted positives, how many were actually correct?

- ‚úÖ Useful when: **False positives** are costly (e.g., spam detection).

---

## üì• Recall (Sensitivity / True Positive Rate)

- **Formula**:  
  $$
  \text{Recall} = \frac{TP}{TP + FN}
  $$

- **Meaning**:  
  Out of all actual positives, how many did we correctly identify?

- ‚úÖ Useful when: **False negatives** are costly (e.g., disease detection).

---

## ‚öñÔ∏è F1 Score

- **Formula**:  
  $$
  \text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
  $$

- **Meaning**:  
  Harmonic mean of Precision and Recall. A balance between them.

- ‚úÖ Use when: You need a **balance** between precision and recall.

---

## üß† Quick Summary Table

| Metric    | Formula                                         | Best for...                            |
| --------- | ----------------------------------------------- | -------------------------------------- |
| Accuracy  | (TP + TN) / (TP + TN + FP + FN)                 | Balanced datasets                      |
| Precision | TP / (TP + FP)                                  | Avoiding False Positives               |
| Recall    | TP / (TP + FN)                                  | Avoiding False Negatives               |
| F1 Score  | 2 √ó (Precision √ó Recall) / (Precision + Recall) | Trade-off between Precision and Recall |

---

## üìå Bonus: Macro vs Micro Averages (for multi-class)

- **Micro**: Calculates metrics **globally** (total TP, FP, FN).
- **Macro**: Calculates metrics **per class**, then takes the average (equal weight to all classes).

